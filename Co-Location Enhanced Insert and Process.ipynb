{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# encoding=utf8  \n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "import sys \n",
    "import json\n",
    "from pprint import pprint\n",
    "import string\n",
    "import io\n",
    "from random import randint\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "class coLocation:\n",
    "    \n",
    "    text_data_list = []\n",
    "    vocab_list = []\n",
    "    corpus_list = []\n",
    "    data_dir = \"data/\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "    \n",
    "    def read80legsData(self, json_file_path):\n",
    "        text_data_list = []\n",
    "        with open(json_file_path) as data_file:    \n",
    "            json_data = json.load(data_file)\n",
    "            for json_obj in json_data:\n",
    "                try:\n",
    "                    pre_text = json.loads(json_obj['result'])\n",
    "                    self.text_data_list.append(pre_text['text'].encode(encoding='UTF-8',errors='ignore'))\n",
    "                except:\n",
    "                    temp = ''\n",
    "                \n",
    "        return self.text_data_list\n",
    "        \n",
    "    def extractVocab(self, text_list):\n",
    "        self.vocab_list = []\n",
    "        for text_item in text_list:\n",
    "            text_item = text_item.encode('utf-8')\n",
    "            replace_punctuation = string.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "            text_item = text_item.translate(replace_punctuation)\n",
    "\n",
    "            vocab_temp_list = text_item.split()\n",
    "            \n",
    "            # filter the list \n",
    "            #get rid of mixed case\n",
    "            # most words are less tha 10 letters long\n",
    "            # being generous and allowing for 15\n",
    "            # see http://norvig.com/mayzner.html\n",
    "            vocab = [elem for elem in vocab_temp_list if sum(1 for c in elem if c.isupper()) < 3 \n",
    "                     and len(elem) < 20]\n",
    "            # make all lowercase \n",
    "            vocab = [x.lower() for x in vocab]\n",
    "            # filter out any spaces left in list\n",
    "            vocab = [x.replace(\" \",\"\") for x in vocab]\n",
    "            vocab = [x.decode('utf-8') for x in vocab]\n",
    "            vocab = [x.replace(\"\\\\u2019\",\"'\") for x in vocab]\n",
    "            \n",
    "            self.corpus_list = self.corpus_list + vocab\n",
    "            \n",
    "            vocab = set(vocab)\n",
    "            vocab = list(vocab)\n",
    "            \n",
    "            self.vocab_list = self.vocab_list + vocab\n",
    "         \n",
    "        return self.vocab_list\n",
    "    \n",
    "    def cleanTextList(self, textList):\n",
    "        for idx, text in enumerate(textList):\n",
    "            textList[idx] = re.sub('\\d', ' ', text) # remove numbers\n",
    "            \n",
    "            # remove punctuation and deal with encoding \n",
    "            textList[idx] = text.encode(encoding='UTF-8',errors='ignore').translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "            textList[idx] = text.encode('ascii',errors='ignore') # remove non ascii characters\n",
    "            \n",
    "            # remove mixed case words\n",
    "            mixed_case = re.findall(\"(?:[A-Z])(?:\\S?)+(?:[A-Z])(?:[a-z])+\",  text) # find mixed case\n",
    "            big_regex = re.compile('|'.join(map(re.escape, mixed_case))) # regex for mixed case words\n",
    "            textList[idx] = big_regex.sub(\" \", text) # replace mixed case words\n",
    "            \n",
    "            #remove all words longer than x\n",
    "            tooLongWords = re.findall(\"\\w{20,}\", text)\n",
    "            big_regex = re.compile('|'.join(map(re.escape, tooLongWords))) # regex for mixed case words\n",
    "            textList[idx] = big_regex.sub(\" \", text) # replace mixed case words\n",
    "            \n",
    "            #textList[idx] = text.lower () # make lowercase\n",
    "            \n",
    "        return textList\n",
    "    \n",
    "    def getCoLocations():\n",
    "        temp = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coLoc = coLocation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data from 80legs.com data parsed and json'ed to file\n",
    "with io.open('data/json_parsed/wallStJ_1.json', 'w') as f:\n",
    "  f.write(unicode(json.dumps(coLoc.read80legsData(\"data/wallStJ_1.txt\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/json_parsed/wallStJ_1.json') as data_file:    \n",
    "    text_data = json.load(data_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_data = coLoc.cleanTextList(text_data[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ngram_data = []\n",
    "n = 2\n",
    "sixgrams = ngrams(text_data[0].split(), n)\n",
    "for grams in sixgrams:\n",
    "  ngram_data.append(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'N', u'i')]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_data[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "c = Counter( ngram_data )\n",
    "\n",
    "count_data = c.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'&', u'O'), 1),\n",
       " ((u'e', u'&'), 6),\n",
       " ((u',', u'N'), 1),\n",
       " ((u'U', u'R'), 1),\n",
       " ((u'n', u'C'), 10),\n",
       " ((u'L', u'o'), 16),\n",
       " ((u'w', u'M'), 1),\n",
       " ((u'.', u'A'), 7),\n",
       " ((u'c', u'u'), 8),\n",
       " ((u'\\xe9', u'r'), 4),\n",
       " ((u'S', u'W'), 1),\n",
       " ((u'J', u'u'), 2),\n",
       " ((u'C', u'-'), 2),\n",
       " ((u'o', u'T'), 3),\n",
       " ((u'y', u'w'), 1),\n",
       " ((u'h', u's'), 2),\n",
       " ((u'h', u'D'), 1),\n",
       " ((u'n', u'o'), 24),\n",
       " ((u'l', u':'), 1),\n",
       " ((u'a', u'x'), 1),\n",
       " ((u'l', u'A'), 3),\n",
       " ((u'|', u'F'), 1),\n",
       " ((u'd', u'g'), 2),\n",
       " ((u's', u'A'), 18),\n",
       " ((u'Y', u'r'), 1),\n",
       " ((u'E', u'U'), 2),\n",
       " ((u'z', u'z'), 2),\n",
       " ((u'A', u'N'), 1),\n",
       " ((u't', u'z'), 1),\n",
       " ((u'o', u'x'), 1),\n",
       " ((u's', u'o'), 35),\n",
       " ((u'V', u'i'), 29),\n",
       " ((u'S', u'p'), 7),\n",
       " ((u't', u'D'), 7),\n",
       " ((u'l', u'P'), 2),\n",
       " ((u'E', u'x'), 10),\n",
       " ((u'h', u'E'), 2),\n",
       " ((u't', u'E'), 3),\n",
       " ((u'l', u'M'), 2),\n",
       " ((u'E', u'c'), 10),\n",
       " ((u'J', u'S'), 1),\n",
       " ((u'C', u'o'), 71),\n",
       " ((u'P', u'e'), 16),\n",
       " ((u'o', u'A'), 1),\n",
       " ((u'\\u201c', u'T'), 1),\n",
       " ((u'f', u'$'), 1),\n",
       " ((u'l', u'.'), 1),\n",
       " ((u'|', u'S'), 1),\n",
       " ((u'a', u'W'), 1),\n",
       " ((u'.', u'5'), 5),\n",
       " ((u'o', u'S'), 10),\n",
       " ((u'g', u\"'\"), 3),\n",
       " ((u'&', u'&'), 1),\n",
       " ((u'v', u's'), 3),\n",
       " ((u'1', u'c'), 1),\n",
       " ((u'-', u'd'), 1),\n",
       " ((u'k', u'y'), 4),\n",
       " ((u'&', u'T'), 3),\n",
       " ((u's', u'H'), 12),\n",
       " ((u'l', u'?'), 1),\n",
       " ((u'g', u'p'), 3),\n",
       " ((u'y', u'\\u2014'), 1),\n",
       " ((u'O', u'i'), 3),\n",
       " ((u'f', u'c'), 2),\n",
       " ((u'w', u'Y'), 1),\n",
       " ((u's', u'|'), 1),\n",
       " ((u'l', u'\\u4e2d'), 4),\n",
       " ((u'A', u'i'), 2),\n",
       " ((u'H', u'o'), 20),\n",
       " ((u'l', u'Y'), 4),\n",
       " ((u',', u'b'), 6),\n",
       " ((u'2', u'1'), 2),\n",
       " ((u'\\u2018', u'S'), 1),\n",
       " ((u'l', u'N'), 1),\n",
       " ((u'K', u'r'), 1),\n",
       " ((u'S', u'e'), 35),\n",
       " ((u't', u'g'), 2),\n",
       " ((u'Y', u'e'), 1),\n",
       " ((u'p', u'T'), 1),\n",
       " ((u'f', u'r'), 7),\n",
       " ((u'T', u'e'), 20),\n",
       " ((u'n', u'H'), 8),\n",
       " ((u'J', u'/'), 2),\n",
       " ((u'S', u'S'), 2),\n",
       " ((u'4', u'0'), 1),\n",
       " ((u'w', u'P'), 1),\n",
       " ((u'k', u'p'), 1),\n",
       " ((u'm', u'i'), 20),\n",
       " ((u'r', u'i'), 110),\n",
       " ((u'n', u','), 7),\n",
       " ((u'%', u'H'), 1),\n",
       " ((u'r', u'w'), 1),\n",
       " ((u':', u'O'), 1),\n",
       " ((u'\\u2013', u'A'), 1),\n",
       " ((u'd', u'n'), 1),\n",
       " ((u'y', u'd'), 3),\n",
       " ((u's', u'S'), 26),\n",
       " ((u'a', u'L'), 7),\n",
       " ((u'a', u's'), 77),\n",
       " ((u'z', u'l'), 2)]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(count_data)\n",
    "count_data[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['teSto']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vocab = coLoc.extractVocab(text_data)\n",
    "texttest = \"thisIsAtest Test teSto U.S.A\"\n",
    "mixed_case = re.findall(\"(?:[A-Z])(?:\\S?)+(?:[A-Z])(?:[a-z])+\",  texttest)\n",
    "\n",
    "big_regex = re.compile('|'.join(map(re.escape, mixed_case)))\n",
    "texttest = big_regex.sub(\" \", texttest)\n",
    "\n",
    "texttest  = re.findall(\"\\w{5,}\", texttest)\n",
    "\n",
    "texttest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data parsed into vocab and json'ed to file\n",
    "with io.open('data/json_parsed/wallStJ_1.vocab.json', 'w') as f:\n",
    "  f.write(unicode(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
