{
 "metadata": {
  "name": "",
  "signature": "sha256:09550abea66289e91e208f240bc2dc4e552c1fdcd639ad3e9054a3c68c57c1c5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "import nltk\n",
      "from nltk.util import ngrams\n",
      "from HTMLParser import HTMLParser \n",
      "import re\n",
      "import json\n",
      "    \n",
      "# from http://stackoverflow.com/questions/753052/strip-html-from-strings-in-python\n",
      "# by Peter Mortensen\n",
      "class MLStripper(HTMLParser):\n",
      "    def __init__(self):\n",
      "        self.reset()\n",
      "        self.fed = []\n",
      "    def handle_data(self, d):\n",
      "        self.fed.append(d)\n",
      "    def get_data(self):\n",
      "        return ''.join(self.fed)\n",
      "\n",
      "def strip_tags(html):\n",
      "    s = MLStripper()\n",
      "    s.feed(html)\n",
      "    return s.get_data()\n",
      "\n",
      "def get_text(url_list):\n",
      "    page_text = ''\n",
      "    \n",
      "    for page_url in url_list:\n",
      "        html = urllib.urlopen(page_url).read()\n",
      "        soup = BeautifulSoup(html)\n",
      "        body = soup.find('body')\n",
      "        [s.extract() for s in body(['iframe', 'script'])]\n",
      "        \n",
      "        page_text += body.text+\" \"\n",
      "        \n",
      "    page_text = page_text.replace(\"\\(\",\" \").replace(\"\\)\",\" \").replace(u'\\u00bb', u' ')\n",
      "    page_text = page_text.replace(\"\\[\",\" \").replace(\"\\]\",\" \").replace(\"|\",\" \")\n",
      "    page_text = page_text.replace(\".\",\" \").replace(\",\",\" \")\n",
      "    page_text = \" \".join(re.findall('[A-Z][^A-Z]*', page_text)).lower()\n",
      "    \n",
      "    \n",
      "    return page_text\n",
      "\n",
      "def get_ngram_list(text_body, n):\n",
      "    ngram_list = []\n",
      "    sixgrams = ngrams(text_body.split(), n)\n",
      "    for grams in sixgrams:\n",
      "        ngram_list.append(grams)\n",
      "    return ngram_list\n",
      "\n",
      "def get_joined(joinee_list):\n",
      "    joined = []\n",
      "    for join_item in joinee_list:\n",
      "        joined.append(' '.join(join_item))\n",
      "    return joined       \n",
      "\n",
      "def get_ngram_counts(ngram_joined, min_count):\n",
      "    ngram_counts = {}    \n",
      "    for ngram_item in ngram_joined:\n",
      "        if ngram_joined.count(ngram_item) >= min_count:\n",
      "            ngram_counts[ngram_item] = ngram_joined.count(ngram_item)\n",
      "    return ngram_counts\n",
      "\n",
      "def extract_ngrams(data_input,min_count):\n",
      "    ngram_counts = {}\n",
      "    if isinstance(data_input, list):\n",
      "        page_text = get_text(data_input) \n",
      "    else:\n",
      "        page_text = data_input\n",
      "        \n",
      "    noun_ngram_counts = extract_noun_ngrams(page_text,min_count)\n",
      "    \n",
      "    # n2-n6   \n",
      "    n_count = 2\n",
      "    while n_count < 7:\n",
      "        ngram_list = get_ngram_list(page_text,n_count)\n",
      "        ngram_joined = get_joined(ngram_list)\n",
      "        ngram_set = get_ngram_counts(ngram_joined,min_count)\n",
      "        ngram_counts[n_count] = ngram_set\n",
      "        n_count += 1\n",
      "        \n",
      "    ngram_count_results = {\"ngram_counts\":ngram_counts, \"noun_ngram_counts\":noun_ngram_counts}    \n",
      "    return json.dumps(ngram_count_results) \n",
      "    \n",
      "# returns mostly nouns    \n",
      "def extract_noun_ngrams(page_text,min_count):\n",
      "\n",
      "    ngram_counts = {}\n",
      "    text = nltk.word_tokenize(page_text)\n",
      "    pos_text = nltk.pos_tag(text)\n",
      "    noun_text = \"\"\n",
      "    for word,pos in pos_text:\n",
      "        if \"IN\" not in pos and \"N\" in pos:\n",
      "            noun_text += word+\" \"\n",
      "    # n2-n6   \n",
      "    n_count = 2\n",
      "    while n_count < 7:\n",
      "        ngram_list = get_ngram_list(noun_text,n_count)\n",
      "        ngram_joined = get_joined(ngram_list)\n",
      "        ngram_set = get_ngram_counts(ngram_joined,min_count)\n",
      "        ngram_counts[n_count] = ngram_set\n",
      "        n_count += 1\n",
      "        \n",
      "    return ngram_counts "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print extract_ngrams(\"a a a a d d gg f s  a e f ews s s sss s ssss s sss dog dog dog\",2)\n",
      "#print get_text(['http://news.google.com'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{\"ngram_counts\": {\"2\": {\"dog dog\": 2, \"a a\": 3, \"s sss\": 2}, \"3\": {\"a a a\": 2}, \"4\": {}, \"5\": {}, \"6\": {}}, \"noun_ngram_counts\": {\"2\": {\"dog dog\": 2}, \"3\": {}, \"4\": {}, \"5\": {}, \"6\": {}}}\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 165,
       "text": [
        "'The Long And Winding Road'"
       ]
      }
     ],
     "prompt_number": 165
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print extract_ngrams(get_text(['http://news.google.com']),3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}