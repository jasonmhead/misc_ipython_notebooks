{
 "metadata": {
  "name": "",
  "signature": "sha256:8393bffd605c6be3d7ff65efd62e4b3ec12452ad3e56bcdfd8dd150d316d7537"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "url = \"http://www.gutenberg.org/files/2554/2554.txt\"\n",
      "response = urllib2.urlopen(url)\n",
      "raw = response.read().decode('utf8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 584
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"cologne.txt\",\"r\")\n",
      "raw = f.read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from collections import defaultdict\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "def get_word_distances_per_sentence(text_to_parse, target_word, sentence_or_phrase, use_stopwords=1):\n",
      "    \n",
      "    if use_stopwords == 1:\n",
      "        stopwords = nltk.corpus.stopwords.words('english')\n",
      "    else:\n",
      "        stopwords = []\n",
      "    \n",
      "    # clean up #\n",
      "    target_word = target_word.lower()\n",
      "    text_to_parse = clean_text(text_to_parse)\n",
      "    \n",
      "    if sentence_or_phrase == 'sentence':\n",
      "        text_to_parse = text_to_parse.replace(',',' ').replace('  ',' ')\n",
      "    # end cleanup #\n",
      "    \n",
      "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "    sentences = tokenizer.tokenize(text_to_parse)\n",
      "    \n",
      "    if sentence_or_phrase == 'phrase':\n",
      "        # split into phrases\n",
      "        base_segments = []\n",
      "        # split by commas\n",
      "        for sentence in sentences:\n",
      "            \n",
      "            # replace other phrase punctuation with commas, and clean up\n",
      "            sentence = sentence.replace(\":\",\",\").replace(\";\",\",\").replace(\" ,\",\",\").replace(\", \",\",\")\n",
      "            \n",
      "            base_segments.append(sentence.split(','))\n",
      "        sentences = flatten_list(base_segments)\n",
      "    \n",
      "    word_distance_sets = []\n",
      "    \n",
      "    \n",
      "    # remove sentence if the taget word is not in the sentence\n",
      "    valid_sentences = []\n",
      "    for sentence in sentences:\n",
      "        # test for a target word match # a space before or after\n",
      "        target_word_pre = \" \"+target_word\n",
      "        target_word_post = target_word+\" \"\n",
      "        if sentence.find(target_word_pre) != -1 or sentence.find(target_word_post) != -1:\n",
      "            valid_sentences.append(sentence)\n",
      "    sentences = valid_sentences\n",
      "    \n",
      "    # return empty if there is no match for the target word\n",
      "    if len(sentences) == 0:\n",
      "        return {}\n",
      "    \n",
      "    for sent_idx, sentence in enumerate(sentences):    \n",
      "        word_distances = {}\n",
      "        sentence = sentence.strip('.')\n",
      "        words = sentence.split(' ')\n",
      "        \n",
      "        # get all word positions of the target word in sentence \n",
      "        target_word_positions = [i for i, x in enumerate(words) if x == target_word]\n",
      "        \n",
      "        \n",
      "        word_distance_sets.append({\"target\":[], \"words\":[]})\n",
      "        \n",
      "        # get the word positions of all the words\n",
      "        for word in words:\n",
      "            #exclude the target word and stopwords\n",
      "            if word != target_word and word not in stopwords:\n",
      "                word_distances[word] = [i for i, x in enumerate(words) if x == word]\n",
      "        word_distance_sets[sent_idx]['target'] = target_word_positions  \n",
      "        word_distance_sets[sent_idx]['words']= word_distances  \n",
      "        \n",
      "    # compute the distances #\n",
      "    \n",
      "    computed_distances_set = []\n",
      "    # for each sentence\n",
      "    for distance_set in word_distance_sets:\n",
      "        computed_distances_set.append(compute_distances(distance_set[\"target\"], distance_set[\"words\"]))\n",
      "    return merge_dicts(computed_distances_set)\n",
      "            \n",
      "def compute_distances(target_set, word_set):\n",
      "    word_distances = {}\n",
      "    # for each target word occurence\n",
      "    for target_position in target_set:\n",
      "        # for each set of word occurances\n",
      "        for word,word_positions in word_set.iteritems():\n",
      "            word_distances[word] = []\n",
      "            # for each word position\n",
      "            for word_position in word_positions:\n",
      "                word_distances[word].append(abs(int(target_position)-int(word_position)))\n",
      "    return word_distances     \n",
      "\n",
      "# adapted from http://stackoverflow.com/questions/5946236/how-to-merge-multiple-dicts-with-same-key\n",
      "def merge_dicts(list_of_dicts): # where \n",
      "    d = {}\n",
      "    for k in list_of_dicts[0]:\n",
      "        d[k] = list(d.get(k, '') for d in list_of_dicts)\n",
      "        \n",
      "    #flatten list \n",
      "    for word,word_list in d.iteritems():\n",
      "        d[word] = flatten_list(word_list)\n",
      "    return d\n",
      "\n",
      "def flatten_list(list_of_lists):\n",
      "    flattened = [item for sublist in list_of_lists for item in sublist]\n",
      "    \n",
      "    return flattened\n",
      "\n",
      "def clean_text(text_to_parse):\n",
      "    text_to_parse = text_to_parse.replace(',',', ').replace('  ',' ').lower()\n",
      "    text_to_parse = text_to_parse.replace('\\r',' ').replace('\\n',' ')\n",
      "    text_to_parse = text_to_parse.replace('...','.').replace('..','.').replace('....','.')\n",
      "    text_to_parse = text_to_parse.replace('?','.').replace('!','.')\n",
      "    text_to_parse = text_to_parse.replace('---',' ').replace('--',' ')\n",
      "    text_to_parse = text_to_parse.replace('*',' ')\n",
      "    text_to_parse = text_to_parse.replace('(',' ').replace(')',' ')\n",
      "    text_to_parse = text_to_parse.replace('[',' ').replace(']',' ')\n",
      "    text_to_parse = text_to_parse.replace('{',' ').replace('}',' ')\n",
      "    text_to_parse = text_to_parse.replace('\"',' ')\n",
      "    text_to_parse = text_to_parse.replace('   ',' ').replace('  ',' ')\n",
      "    return text_to_parse"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "text_to_parse = \"Factoring in all its benefits, the technology could: save the U.S. \"\\\n",
      "\"economy about $450 billion annually, according to an analysis by the \"\\\n",
      "\"Eno Center for Transportation think tank,\"\\\n",
      "\"a nonprofit think tank. The autonomous Navia shuttle, made by the French firm Induct, not in the U.S.A., \"\\\n",
      "\"already shuttles passengers around several; overseas college campuses, while London-based, \"\\\n",
      "\"Rio Tinto operates more than 50 autonomous trucks at Australian mining sites.\"\n",
      "\n",
      "print get_word_distances_per_sentence(text_to_parse, \"while\", \"phrase\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'london-based': [1]}\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print get_word_distances_per_sentence(raw, \"smelling\", \"sentence\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'shot': [188], \"don't\": [22], 'years': [177], 'edt': [69, 31, 35, 28, 32], 'replies': [28, 5, 44, 82, 120, 154, 198, 161, 141, 116, 90, 58, 32, 34, 21, 70, 27, 3, 56, 22, 12, 19, 5, 51, 22, 8, 36, 101, 69, 40, 17, 8, 52, 19, 5, 22, 26, 78, 49, 15, 15, 17, 52, 18, 8, 18, 13, 11, 17, 217, 180, 148, 122, 97, 65, 29, 7, 51, 22], 'go': [34, 12, 12], 'laurel': [19], '\\xe2\\x80\\x8f@laurhellian': [17], 'rose': [91], 'suits': [101], '0': [29, 4, 43, 81, 83, 85, 119, 153, 197, 199, 162, 142, 117, 91, 59, 33, 35, 22, 20, 18, 71, 69, 67, 28, 24, 2, 4, 57, 55, 53, 23, 21, 19, 11, 13, 15, 20, 18, 16, 4, 52, 50, 48, 23, 7, 9, 35, 37, 39, 102, 100, 98, 70, 41, 18, 16, 7, 53, 49, 20, 18, 16, 4, 6, 8, 23, 21, 19, 27, 25, 23, 79, 77, 75, 50, 46, 16, 16, 14, 18, 16, 14, 53, 51, 49, 19, 17, 15, 7, 19, 17, 15, 14, 12, 10, 10, 12, 18, 16, 14, 218, 216, 214, 181, 179, 177, 149, 147, 145, 123, 119, 98, 96, 94, 66, 64, 62, 30, 28, 26, 6, 50, 23, 21, 19], 'http://rover.ebay.com/rover/1/711-53200-19255-0/1.ff3=4&toolid=100034&campid=5337535881&customid=sherifkok&mpre=http://deals.ebay.com/5001701242_aramis_for_men_cologne_spray_3_7_oz_edt_new_in_box': [58], 'retweets': [84, 122, 200, 88, 56, 30, 32, 19, 68, 5, 54, 20, 14, 17, 7, 49, 10, 38, 99, 67, 38, 15, 17, 7, 20, 24, 76, 13, 13, 15, 50, 16, 10, 16, 11, 13, 15, 215, 178, 146, 95, 63, 27, 53, 20], '8': [123, 157, 176, 138, 37, 11, 39, 6, 19], 'scarlett': [15], 'gave': [181], 'dad': [170], '...': [216], '#heartsoup': [118], 'get': [26], 'could': [32, 119], 'ladies\\xe2\\x80\\x9d': [38], 'choice': [40], '\\xe2\\x80\\x8f@brianparinooo': [165], 'know': [23, 152, 80], \"boyfriend's\": [136, 34], 'reply': [23, 10, 49, 87, 125, 159, 203, 156, 136, 111, 85, 53, 27, 29, 16, 65, 22, 8, 51, 17, 17, 14, 10, 46, 17, 13, 41, 96, 64, 35, 12, 13, 47, 14, 10, 17, 21, 73, 44, 10, 10, 12, 47, 13, 13, 13, 8, 16, 12, 212, 175, 143, 117, 92, 60, 24, 12, 56, 17], 'nov': [132, 166, 210, 150, 129, 104, 77, 46, 20, 23, 9, 59, 16, 15, 44, 10, 24, 8, 16, 39, 10, 19, 48, 90, 58, 29, 6, 21, 40, 7, 18, 10], \"'please'\": [117], 'loonyland': [221], 'like': [139, 63, 45, 1, 30, 3, 1], 'http://bit.ly/1zihala': [33], '55': [75], '\\xe2\\x80\\x8f@bootshaus_club': [209], 'bae': [28], 'retweet2': [126, 26, 11, 34], '#deals': [60, 19, 21, 39, 24, 67], 'retweet1': [22, 11, 50, 160, 155, 135, 110, 21, 16, 14, 46, 43, 116, 13], 'becker': [130], 'amanda': [129], 'xmas': [30], 'view': [41], 'parino\\xe2\\x98\\x83': [164], 'tiffany': [14], 'cologne': [1, 37, 65, 107, 141, 185, 208, 224, 146, 122, 99, 61, 36, 1, 1, 3, 50, 1, 24, 44, 1, 5, 1, 28, 59, 75, 43, 22, 3, 24, 1, 26, 5, 10, 1, 57, 28, 1, 1, 26, 1, 35, 5, 33, 1, 3, 37, 6, 1, 195, 103, 40, 44, 73, 1], '3.7': [67], 'world\\xf0\\x9f\\x98\\x8d\\xe2\\x80\\x9d': [149], 'retweet': [26, 7, 46, 88, 156, 204, 159, 139, 114, 15, 64, 25, 9, 50, 16, 18, 13, 45, 20, 14, 42, 95, 11, 10, 50, 13, 11, 16, 20, 72, 47, 9, 11, 46, 12, 12, 7, 17, 11, 211, 174, 142, 120, 91, 59, 23, 9, 16], 'best': [1, 39, 53, 112, 145, 184, 219, 148, 123, 96, 75, 37, 1, 1, 1, 43, 14, 12, 35, 1, 29, 1, 37, 1, 31, 52, 77, 56, 24, 6, 27, 1, 15, 1, 1, 63, 25, 1, 1, 4, 1, 17, 1, 7, 39, 1, 200, 167, 133, 107, 78, 44, 1, 25, 61, 1], 'me:': [150], 'said': [116], 'bootshaus': [207], '3': [8, 158, 113, 57, 29, 31], '\\xe2\\x80\\x8f@best_tips_443': [55, 14, 19], 'new': [70, 104, 34, 42, 8, 7, 35, 36, 25, 34, 203, 186, 132, 81], 'tips': [54, 13, 18], 'red': [92], '@steveaoki': [215], 'never': [33], 'men': [64, 25, 43, 28, 34], 'spray': [66, 4, 4, 32, 79, 34, 27], 'hoodie': [137, 66, 81], '\\xf0\\x9f\\x91\\x8c\\xf0\\x9f\\x99\\x87': [2], 'fantasies': [110], 'box': [72, 38, 36], \"wouldn't\": [151], '30': [133, 167, 211, 149], 'favorite': [24, 48, 89, 14, 63, 20, 49, 15, 19, 12, 44, 43, 94, 45, 12, 12, 15, 19, 71, 42, 10, 45, 11, 11, 6, 10, 210, 173, 141, 115, 90, 58, 22, 11, 15], 'favorite42': [205], 'conversation': [42], '\\xe2\\x80\\x8f@geletilari': [93], 'oz': [68, 28, 46, 30, 33, 31, 31, 80], '\\xe2\\x80\\x9c@eliep_:': [19], '$32.14': [73], 'comes': [11], 'first': [173], 'adidas': [7], 'image': [31, 78, 144, 74, 25, 9, 23, 105, 20, 26, 30, 82, 52, 18, 56, 21, 221, 4, 26], 'manners': [113], 'vote': [35], 'crew': [225], 'smells': [138, 64, 46, 56, 3, 3], '\\xe2\\x80\\x8f@amandabeckerrr': [131], '#0115': [61, 20, 25], '2': [15, 121, 31, 6, 45, 6, 19, 39, 12, 204, 168, 134, 110, 84, 52, 16, 10, 20, 65], 'now:': [34], 'today': [178], 'happy': [212, 15], 'way': [196, 25], 'took': [186], 'aramis': [62], '\\xe2\\x80\\x8f@tiffanyyyyys_': [16], 'embedded': [32, 77, 145, 75, 26, 8, 24, 106, 21, 27, 31, 83, 53, 19, 57, 22, 222, 3, 27], 'car': [105], 'sent': [192], 'dec': [16, 17, 56, 94], 'called': [97], 'launched': [109], '$59.99': [74], 'favorite1': [21, 51], 'friend:': [134], 'favorite3': [12, 154, 109, 25, 27], 'skrober': [18], 'moves': [6], 'favorite8': [127, 161, 134, 33, 15], 'thing': [146, 74, 13, 1], '42': [201], '#handsdown': [3], 'gentlemen': [100], 'brian': [163], 'smell': [106, 95, 31, 23], 'a.': [76], 'permalink': [30, 79, 143, 73, 24, 10, 22, 104, 19, 25, 29, 81, 51, 17, 55, 20, 220, 5, 25], 'wrong': [35], 'birthday': [213, 14], 'favorites': [9, 86, 124, 158, 202, 157, 137, 112, 86, 54, 28, 30, 17, 66, 23, 7, 52, 18, 16, 15, 9, 47, 18, 12, 40, 97, 65, 36, 13, 12, 48, 15, 9, 18, 22, 74, 45, 11, 11, 13, 48, 14, 12, 14, 9, 15, 13, 213, 176, 144, 118, 93, 61, 25, 11, 55, 18], '\\xe2\\x80\\xa6': [59, 24, 18, 33, 29, 23], 'date': [174, 6], 'expand': [80, 23, 72, 21, 53, 103, 24, 28, 80, 54, 20, 19, 219, 182, 150, 24], 'forever': [4], 'well': [168], '1': [27, 25, 6, 18, 45, 47, 57, 95, 155, 160, 140, 115, 26, 35, 21, 9, 51, 48, 121, 8]}\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 558
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 539
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}